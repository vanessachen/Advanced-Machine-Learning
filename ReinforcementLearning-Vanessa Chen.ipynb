{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't gone through [Carl's Tutorial](https://github.com/carlshan/intro_to_machine_learning/blob/master/lessons/Reinforcement_Learning/RL_Tutorial.md), you may want to go through that and implement the hill-climbing policy before attempting this tutorial. I would not recommend implementing the policy gradient strategy unless you're interested in learning more about lower-level tensorflow. This tutorial uses the keras, like the others this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from [https://gist.github.com/EderSantana/c7222daa328f0e885093](https://gist.github.com/EderSantana/c7222daa328f0e885093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To be able to run the animation below, make sure you have the latest version of matplotlib, by running `pip3 install matplotlib --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the game environment and replay classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        '''\n",
    "        Input: grid_size (length of the side of the canvas)\n",
    "        \n",
    "        Initializes internal state.\n",
    "        '''\n",
    "        self.grid_size = grid_size\n",
    "        self.min_basket_center = 1\n",
    "        self.max_basket_center = self.grid_size-2\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Moves basket according to action. Moves fruit down. Updates state to reflect these movements\n",
    "        '''\n",
    "        if action == 0:  # left\n",
    "            movement = -1\n",
    "        elif action == 1:  # stay\n",
    "            movement = 0\n",
    "        elif action == 2: # right\n",
    "            movement = 1\n",
    "        else:\n",
    "            raise Exception('Invalid action {}'.format(action))\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        # move the basket unless this would move it off the edge of the grid\n",
    "        new_basket_center = min(max(self.min_basket_center, basket_center + movement), self.max_basket_center)\n",
    "        # move fruit down\n",
    "        fruit_y += 1\n",
    "        out = np.asarray([fruit_x, fruit_y, new_basket_center])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        '''\n",
    "        Returns a 2D numpy array with 1s (white squares) at the locations of the fruit and basket and\n",
    "        0s (black squares) everywhere else.\n",
    "        '''\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        canvas = np.zeros(im_size)\n",
    "        \n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        canvas[fruit_y, fruit_x] = 1  # draw fruit\n",
    "        canvas[-1, basket_center-1:basket_center + 2] = 1  # draw 3-pixel basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        '''\n",
    "        Returns 1 if the fruit was caught, -1 if it was dropped, and 0 if it is still in the air.\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        if fruit_y == self.grid_size-1:\n",
    "            if abs(fruit_x - basket_center) <= 1:\n",
    "                return 1 # it caught the fruit\n",
    "            else:\n",
    "                return -1 # it dropped the fruit\n",
    "        else:\n",
    "            return 0 # the fruit is still in the air\n",
    "\n",
    "    def observe(self):\n",
    "        '''\n",
    "        Returns the current canvas, as a 1D array.\n",
    "        '''\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Returns:\n",
    "            current canvas (as a 1D array)\n",
    "            reward received after this action\n",
    "            True if game is over and False otherwise\n",
    "        '''\n",
    "        self._update_state(action)\n",
    "        observation = self.observe()\n",
    "        reward = self._get_reward()\n",
    "        game_over = (reward != 0) # if the reward is zero, the fruit is still in the air\n",
    "        return observation, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Updates internal state\n",
    "            fruit in a random column in the top row\n",
    "            basket center in a random column\n",
    "        '''\n",
    "        fruit_x = random.randint(0, self.grid_size-1)\n",
    "        fruit_y = 0\n",
    "        basket_center = random.randint(self.min_basket_center, self.max_basket_center)\n",
    "        self.state = np.asarray([fruit_x, fruit_y, basket_center])\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''\n",
    "        Input:\n",
    "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
    "            game_over: boolean\n",
    "        Add the states and game over to the internal memory array. If the array is longer than\n",
    "        self.max_memory, drop the oldest memory\n",
    "        '''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        '''\n",
    "        Randomly chooses batch_size memories, possibly repeating.\n",
    "        For each of these memories, updates the models current best guesses about the value of taking a\n",
    "            certain action from the starting state, based on the reward received and the model's current\n",
    "            estimate of how valuable the new state is.\n",
    "        '''\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1] # the number of possible actions\n",
    "        env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
    "        input_size = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((input_size, env_dim))\n",
    "        targets = np.zeros((input_size, num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
    "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # Set the input to the state that was observed in the game before an action was taken\n",
    "            inputs[i:i+1] = starting_observation\n",
    "            \n",
    "            # Start with the model's current best guesses about the value of taking each action from this state\n",
    "            targets[i] = model.predict(starting_observation)[0]\n",
    "            \n",
    "            # Now we need to update the value of the action that was taken                      \n",
    "            if game_over: \n",
    "                # if the game is over, give the actual reward received\n",
    "                targets[i, action_taken] = reward_received\n",
    "            else:\n",
    "                # if the game is not over, give the reward received (always zero in this particular game)\n",
    "                # plus the maximum reward predicted for state we got to by taking this action (with a discount)\n",
    "                Q_sa = np.max(model.predict(new_observation)[0])\n",
    "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions for creating, training, and visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "epsilon = .1  # probability of exploration (choosing a random action instead of the current best one)\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 50\n",
    "grid_size = 10\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "     Returns three initialized objects: the model, the environment, and the replay.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.2), \"mse\")\n",
    "\n",
    "    # Define environment/game\n",
    "    env = Catch(grid_size)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "    \n",
    "    return model, env, exp_replay\n",
    "\n",
    "\n",
    "def train_model(model, env, exp_replay, num_episodes):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model, env, and exp_replay objects as returned by build_model\n",
    "        num_episodes: integer, the number of episodes that should be rolled out for training\n",
    "    '''\n",
    "    catch_count = 0\n",
    "    for episode in range(num_episodes):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        starting_observation = env.observe()\n",
    "\n",
    "        while not game_over:\n",
    "            # get next action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                # epsilon of the time, we just choose randomly\n",
    "                action = np.random.randint(0, num_actions, size=1)\n",
    "            else:\n",
    "                # find which action the model currently thinks is best from this state\n",
    "                q = model.predict(starting_observation)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            # apply action, get rewards and new state\n",
    "            new_observation, reward, game_over = env.act(action)\n",
    "            if reward == 1:\n",
    "                catch_count += 1\n",
    "\n",
    "            # store experience\n",
    "            exp_replay.remember([starting_observation, action, reward, new_observation], game_over)\n",
    "\n",
    "            # get data updated based on the stored experiences\n",
    "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "            # train model on the updated data\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "\n",
    "            starting_observation = new_observation # for next time through the loop\n",
    "\n",
    "        # Print update from this episode\n",
    "        print(\"Episode {:04d}/{:04d} | Loss {:.4f} | Catch count {}\".format(episode, num_episodes-1, loss, catch_count))\n",
    "\n",
    "\n",
    "def create_animation(model, env, num_games):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model and env objects as returned from build_model\n",
    "        num_games: integer, the number of games to be included in the animation\n",
    "        \n",
    "    Returns: a matplotlib animation object\n",
    "    '''\n",
    "    # Animation code from \n",
    "    # https://matplotlib.org/examples/animation/dynamic_image.html\n",
    "    # https://stackoverflow.com/questions/35532498/animation-in-ipython-notebook/46878531#46878531\n",
    "    \n",
    "    # First, play the games and collect all of the images for each observed state\n",
    "    observations = []\n",
    "    for _ in range(num_games):\n",
    "        env.reset()\n",
    "        observation = env.observe()\n",
    "        observations.append(observation)\n",
    "        game_over = False\n",
    "        while game_over == False:\n",
    "            q = model.predict(observation)\n",
    "            action = np.argmax(q[0])\n",
    "            \n",
    "            # apply action, get rewards and new state\n",
    "            observation, reward, game_over = env.act(action)\n",
    "            observations.append(observation)\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    image = plt.imshow(np.zeros((grid_size, grid_size)),interpolation='none', cmap='gray', animated=True, vmin=0, vmax=1)\n",
    "    \n",
    "    def animate(observation):\n",
    "        image.set_array(observation.reshape((grid_size, grid_size)))\n",
    "        return [image]\n",
    "   \n",
    "    animation = matplotlib.animation.FuncAnimation(fig, animate, frames=observations, blit=True, )\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create our model and game environment and see how it does before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, env, exp_replay = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FuncAnimation' object has no attribute 'to_jshtml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2cfaa74ddfe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manimation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FuncAnimation' object has no attribute 'to_jshtml'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1156211d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animation = create_animation(model, env, num_games=10)\n",
    "IPython.display.HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\r\n",
      "appnope (0.1.0)\r\n",
      "atari-py (0.1.1)\r\n",
      "bleach (1.5.0)\r\n",
      "boto (2.48.0)\r\n",
      "boto3 (1.5.20)\r\n",
      "botocore (1.8.34)\r\n",
      "bz2file (0.98)\r\n",
      "certifi (2018.1.18)\r\n",
      "chardet (3.0.4)\r\n",
      "cycler (0.10.0)\r\n",
      "decorator (4.1.2)\r\n",
      "docutils (0.14)\r\n",
      "entrypoints (0.2.3)\r\n",
      "enum34 (1.1.6)\r\n",
      "future (0.16.0)\r\n",
      "gensim (3.2.0)\r\n",
      "gym (0.9.4)\r\n",
      "h5py (2.7.1)\r\n",
      "html5lib (0.9999999)\r\n",
      "idna (2.6)\r\n",
      "ipykernel (4.6.1)\r\n",
      "ipython (6.1.0)\r\n",
      "ipython-genutils (0.2.0)\r\n",
      "ipywidgets (7.0.0)\r\n",
      "jedi (0.10.2)\r\n",
      "Jinja2 (2.9.6)\r\n",
      "jmespath (0.9.3)\r\n",
      "jsonschema (2.6.0)\r\n",
      "jupyter (1.0.0)\r\n",
      "jupyter-client (5.1.0)\r\n",
      "jupyter-console (5.2.0)\r\n",
      "jupyter-core (4.3.0)\r\n",
      "Keras (2.1.1)\r\n",
      "kiwisolver (1.0.1)\r\n",
      "Markdown (2.6.11)\r\n",
      "MarkupSafe (1.0)\r\n",
      "matplotlib (2.2.0)\r\n",
      "mistune (0.7.4)\r\n",
      "mpmath (1.0.0)\r\n",
      "nbconvert (5.3.1)\r\n",
      "nbformat (4.4.0)\r\n",
      "notebook (5.0.0)\r\n",
      "numpy (1.14.2)\r\n",
      "olefile (0.44)\r\n",
      "pandas (0.20.3)\r\n",
      "pandocfilters (1.4.2)\r\n",
      "pexpect (4.2.1)\r\n",
      "pickleshare (0.7.4)\r\n",
      "Pillow (4.3.0)\r\n",
      "pip (9.0.1)\r\n",
      "prompt-toolkit (1.0.15)\r\n",
      "protobuf (3.5.1)\r\n",
      "ptyprocess (0.5.2)\r\n",
      "pydot (1.2.3)\r\n",
      "pyglet (1.3.0)\r\n",
      "Pygments (2.2.0)\r\n",
      "pynverse (0.1.4.4)\r\n",
      "PyOpenGL (3.1.0)\r\n",
      "pyparsing (2.2.0)\r\n",
      "python-dateutil (2.7.0)\r\n",
      "pytz (2018.3)\r\n",
      "PyYAML (3.12)\r\n",
      "pyzmq (16.0.2)\r\n",
      "qtconsole (4.3.1)\r\n",
      "requests (2.18.4)\r\n",
      "s3transfer (0.1.12)\r\n",
      "scikit-learn (0.19.0)\r\n",
      "scipy (1.0.0)\r\n",
      "setuptools (38.5.2)\r\n",
      "simplegeneric (0.8.1)\r\n",
      "six (1.11.0)\r\n",
      "smart-open (1.5.6)\r\n",
      "sympy (1.1.1)\r\n",
      "tensorflow (1.4.1)\r\n",
      "tensorflow-tensorboard (0.4.0rc3)\r\n",
      "terminado (0.6)\r\n",
      "testpath (0.3.1)\r\n",
      "tornado (4.5.2)\r\n",
      "traitlets (4.3.2)\r\n",
      "urllib3 (1.22)\r\n",
      "wcwidth (0.1.7)\r\n",
      "webencodings (0.5.1)\r\n",
      "Werkzeug (0.14.1)\r\n",
      "wheel (0.30.0)\r\n",
      "widgetsnbextension (3.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train the model and see how much better it is at catching the fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, env, exp_replay, num_episodes=1000)\n",
    "animation = create_animation(model, env, num_games=10)\n",
    "IPython.display.HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Exercises:\n",
    "1. Add a validation function, to make it easier to assess models.\n",
    "1. We've been looking at convolutional neural networks for image processing. Try changing the model to use convolutional layers. Does this seem to work any better?\n",
    "1. Change the code so that the basket is trying to avoid getting hit by the fruit.\n",
    "1. Change the code so the game keeps going until the basket misses the fruit. (To test that this is working, you may want to change the number of games in the animation to 1.) How should this change the rewards that the model gets?\n",
    "1. Change the game to something entirely different.\n",
    "1. Try changing how the training works, using more of a policy gradient strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
